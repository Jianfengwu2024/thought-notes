<h1>基于对称破缺的自组织机制</h1>
<p>现代深度学习的最重要基石是神经网络。本质上，神经网络是从函数空间经由参数矩阵挑选得到的泛函表示。这个泛函接受输入数据，转换为输出标签。其本质是训练很多的弱表示器，然后让这些弱表示器经由串联及并联来获得更强的表示能力。LeCun Yann 指出，现代神经网络的架构需要根本性的革新。原因在于，神经网络的单个神经元的表示能力太弱，而多个神经元的神经元组虽然表示能力增强，但缺乏一种自上而下的自发形成机理，让神经元发生自组织从而形成真正具有极强表示能力的神经元组。</p>
<p>目前的超大参数的训练，既是大模型的强项，也正是其弱点。ChatGPT的成功，在于其强大的能力让更多的人使用并且对其模型进行正反馈，从而能够在线学习更新。形成一家独大的现象。ChatGPT本质上是 transformer 这个表示器的叠加。transformer 的表示能力很强，主要在于其拥有自注意力机制。但这种机制容易受到大规模偏向性的引导。</p>
<p>尽管在专业性很强的问题上，Chat GPT表现的像个专家。但它在常识性的问题上呈现出随波逐流的现象，表现为经常在简单问题上出现不该有的幻觉。这是一种注意力崩塌的现象。</p>
<p>我们观察到，自注意力真正的问题有以下几个方面：一、虽然它有足够强大的表示能力，但它的搜索空间太大。二、存在过度表达的问题，局域过拟合现象是导致幻觉的主要原因。三、自注意力并不等价于自组织性，神经元组之间的多层级组织仍然是空白，多个transformer 模块仍然是靠大力出奇迹，而不是合理分工合作来学习得到表示。</p>
<p>我们认为，存在一种基于对称性破缺的机制，能够让神经元自发组合成为神经元组，并且可以多层自发组合，形成类似于质子-原子-分子-高分子这样的表达机制。在物理学中，对称性自发破缺是量子场论标准模型的最重要的机制。神经网络和场论具有很多相似的地方，比如神经网络里的损失函数，其实和场论里的拉氏量具有类似的含义。而反向传播机制本质上就是场论里的最小作用量原理。transformer强大的表达能力，其实也正说明了，多个特征之间形成自关联具有更高的表达能力。同样，凝聚态场论中，多体关联往往也会导致非常复杂新颖的现象。相变理论正是描述这样的现象的领域。</p>
<p>对称破缺机制能够很好地让多个神经元自发组织成为有序的集团，从而有比 transformer 更强大的表示能力。不同的是，对称破缺的理论中，由于存在守恒量，所以其搜索空间远远小于随机搜索的transformer空间。这意味着基于对称破缺机制的神经元组合，会用极小的训练代价来获得与transformer神经元类似的性能。另外，对称破缺机制也为神经元组的层级自组织提供了原理性的指导，类似分子经由化学键形成的官能团的机制，本质上也是一种对称破缺的过程。通过多个神经元组的层级自组织，可以获得有更强表达能力的高级复杂功能性的神经元组簇。这样的表达能力，将会是现有的transformer无法比拟的。经由对称破缺机制而形成自组织的神经元组，从理论上就避免了局部过拟合的问题。因为正是由于对称破缺出现了有序稳定的结构，使得神经元组具有自我保护的机制，这就类似人类认知中形成了信念一样。不同的神经元组，其功能是确定的，不会因为故意引导而丧失其本来的功能。甚至，如果神经元组能够形成拓扑上稳定的结构，那么它将完全摆脱幻觉这种问题。所以，从原理层面来说，我们认为，超越transformer的表达能力，通过对称破缺的自组织结构来重塑人工智能的基石，是非常值得研究的方向。从实践层面来说，跟随transformer的脚步追赶ChatGPT，差距只会越来越大。只有从机制层面创新产生出超越 transformer 的新神经元组，才能够实现用最小的代价完成最快的超越。</p>
