# 人工智能算法之道--大语言模型
## 1. 拆解 Transformer

现在的大模型基于transformer算法单元。大家都在这个基座上做优化和进一步开发。transformer本质的算法突破就是自注意力机制和位置编码。当然还包括在之前的图像算法中使用的前馈神经网络和残差网络的一些技术。但突破性的工作仍然是自注意力机制和位置编码。


什么是自注意力机制？


自注意力机制基于三个向量生成，分别是查询（query），键（key），值（value）向量。
由Q向量和K向量的点乘，获得注意力权重矩阵。新的预测值向量则由注意力权重矩阵和旧的值向量相乘得到。这样得到的是各个单词或词组之间的关联强度。

什么是位置编码？


把单词按照正确的语法顺序串联起来，就是位置编码。

说人话：自注意力机制和小学生学组词造句是一样的。给定一些词，根据这些词来造出合理的句子来。如果见过相同的句子，那么就很容易根据已知的词来组合出来。


所以，自注意力机制就是死记硬背，照本宣科。如果计算机见过所有的句子，那它就能够靠死记硬背也能把句子造的有模有样。


如果 Transformer 只有这点本事，那也算不得什么大突破。


Transformer 的第三大组件是前馈神经网络。


它把经由自注意力机制以及位置编码得到的Token，映射到输出层。这里的映射通常是非线性映射。这就给死记硬背的自注意力机制插上了“触类旁通”的翅膀。Transformer 自此开始起飞。


说人话，这个前馈神经网络就是让 Transformer 不仅能造句 ：我喜欢吃苹果。也能造句: 我喜欢吃橙子。


组词-造句-联想，三步走就构建出了大模型的基座：Transformer。


实际上，前馈神经网络本身并不是 Transformer 的开创性工作。反而“死记硬背”的自注意力机制是 Transformer 的核心。自注意力机制实际上是规则训练。训练的目的是在熟悉语言单元之间的关系。


Transformer 的注意力机制实际上是线性的。这是非常重要的事实。线性并不意味着差，非线性也并不意味着更好。线性是保证精确的基础，而非线性是保证扩展的基础。两者各司其职，才造就了Transformer 革命。


## 2. 超越 Transformer ？

### 2.1 Transformer 的原生困难

我们的人工智能算法，本质上是编码层面的10序列。这意味着，它根本不需要声音来传递信息。所以它和人类的语言是很不同的。声音对计算机来说，反而意味着编码无效冗余。人工智能的语言，和大自然的生命是很像的。因为对它来说，表意语言是更高效的，扩展性也是更丰富的。


实际上，表音语言用于训练大语言模型，本身就意味着极大的资源浪费和先天缺陷。因为在这种语言中，信息就是语音本身。它天生不具备更好的组合性。这会导致训练出来的模型本身就有组合推理上的巨大缺陷。这也是幻觉的主要来源。原因是巨大的词库导致了关联的稀释，从而导致模糊异变的语义。

### 2.2 学习大自然的生命语言

大自然的生命语言，从碱基对到密码子，再到基因片段，最后是DNA，则是极端高效的语言体系。


由于都是表意语言，生命语言和汉语有非常多的类似的地方。


我们来看看这些对比有多大的相似性。


碱基对类比到汉语中，就对应于笔画。碱基对的组合，就是密码子。它和汉字中的部首是很类似的。

- 基因作为“段落”：基因是由多个密码子组成的序列，编码特定的蛋白质或功能性RNA，类似于段落传达特定的主题或信息。

- 染色体作为“文章”：染色体是由DNA分子组成的结构，包含许多基因，类似于一篇文章由多个段落组成，传达更全面的信息。

- 人类基因组作为“书”：人类有23对染色体，每对染色体包含大量的基因和遗传信息，类似于一本书由多篇文章组成，涵盖了一个完整的主题或故事。


一个神奇的事实是，汉字的常见偏旁大概有一百个，而常见的独体字则只有几十个。这和密码子有64个，但氨基酸却只有20个也是很类似的。


人类语言和大自然的生命语言当然还是有所不同的。人类语言的组合长度相对较短，通常不会超过十个。但生命体的基因则通常包含上千个密码子。虽然它们通常按照20种氨基酸的形式排列，但一个基因实际上代表的组合序列也要远远超过人类语言的组合。所以其实大自然的基本字（20种氨基酸）少，但是语法结构实际上很多（排列组合的长度很长）。当然，我们可以说密码子实际上类似于文字中的部首。所以类似汉语来表达：“您吃饭了吗”，在生命语言的表达就类似：
“人尔心口乞食反了口马”，显然是冗长了许多。尽管如此，由于生命语言的基本字数少，要有丰富的表示，就需要用更长的序列来表达。


### 2.3 基语言设计

==所以是应该从汉语出发，作为人工智能的基语言吗？== 换言之，将所有的语料都先翻译成汉语，然后再进行推理训练么？


很可惜，并不是。因为现代汉语并不是按照笔画-部首-字来编码的。所以这从本质上就和我们需要的基语言不同。


从大自然的生命之书的基语言构造来看，现在的表音文字根本不适合作为人工智能的基语言。但回到 Transformer 来看，为什么 Transformer 可以有神奇的效果？


虽然大语言模型最大的推动是大力出奇迹。 但这里有一个暗合自然之道的巧合。


Transformer 的核心和密码子是类似的，QKV本质上就是一种密码子的组合。尽管大模型的密码子的自由度远远大于大自然的64种，但在天量的数据训练下，最终核心的密码子会远远低于模型的参数量。否则训练出来的模型就没有稳定的核心。它也正是大模型“涌现”的核心。


那么，如果我们一开始就约束人工智能的密码子维度（比如就设为 64 个），这样设计的基语言天生就是组合语言，它的扩展性和表示能力远远超越表音语言。用一个熟悉的词来形容，这叫降维打击。


一个密码子（部首）就是一个64维独热向量。将其非线性约束到更低的维度（比如20维），这相当于是在造氨基酸（独体字）。然后用这些氨基酸做空间折叠（相当于造更复杂的字）。这也是一部非线性变换，一般有几千种折叠方法（字）。这里暂定为5000种（字）。由这5000字，就可以生成出句子。让这些句子再组合，就可以形成段落。将人类语言作为输入，输出为解码为基语言。然后用生成对抗的方法，来反向输出人类语言。当输入和输出趋同时，可以认为基语言已经生成成功了。


## 3.基于核心基语言的大模型


当基语言构造完成时，可以将它替换掉transformer中的编码器以及解码器，再进行大语料的训练，这就可以基于组合性极强的基语言来获得推理和逻辑能力大幅提升的大语言模型。这种训练方式虽然也需要大量的参数，但是因为每个模块的核心的语言模型已经固定了，所以训练的参数量会大大少于现在的大模型，甚至会有比较大的数量级上的区别。


在推理和逻辑能力上，这样开发的语言模型，幻觉将有望消失。而且对于专业性比较强的领域，可以做到模型的尺寸很小。


